+++
title = "Awesome Performance Testing Tools"
description = "A curated, opinionated list of tools and resources dedicated to Performance Testing as a continuous engineering practice â€” from workload design to results analysis."
weight = 20
date = 2026-02-10
updated = 2026-02-20
toc = true

[taxonomies]
tags = ["performance-testing", "performance-engineering", "awesome-list"]

[extra]
copy_button = true
giscus = false
show_reading_time = false
+++

> A curated, opinionated list of tools and resources dedicated to **Performance Testing** as a continuous engineering practice â€” not just load generation.

Performance testing is about validating that systems behave acceptably under realistic conditions: expected load, peak traffic, degraded infrastructure, and adversarial scenarios. Done well, it provides **confidence before deployment and evidence after incidents**.

This list covers the full performance testing lifecycle â€” from **workload design and data generation** through **load execution and chaos injection** to **results analysis and CI/CD integration**.

---

## ðŸŽ¯ Scope & Intent

This list is curated with the following principles:

* Performance testing as a **continuous discipline**, not a pre-release gate
* Emphasis on **realistic workloads** â€” production-like data, traffic patterns, and failure modes
* Recognition that **shift-left performance** is essential â€” catching regressions in CI, not in staging
* Tools that **complement observability** â€” because load generation without visibility is noise
* Inclusion of **chaos engineering** as integral to performance validation â€” systems must perform under failure, not just under load

The goal is to help practitioners:

* Select tools adapted to their **technology stack, team size, and maturity**
* Build **repeatable, realistic performance test suites** that evolve with the system
* Integrate performance feedback into **delivery pipelines** rather than treating it as a separate phase

---

## ðŸ§­ How to Read This List

Tools are organized by **functional category**, but real performance testing often combines several categories. A complementary reading by use case:

### ðŸ”Ž Performance Testing by Use Case

| Use case | Categories involved | Key tools |
| -------- | ------------------- | --------- |
| **API load testing in CI/CD** | Load testing, CI/CD integration | k6, Gatling, Artillery |
| **Full-stack performance validation** | Load testing, browser perf, observability | k6, Lighthouse, Grafana |
| **Microservice resilience testing** | Chaos engineering, service virtualization | Litmus, Gremlin, WireMock |
| **Database performance benchmarking** | Benchmarking, data generation | HammerDB, sysbench, pgbench |
| **Frontend experience optimization** | Browser performance, RUM | Lighthouse, WebPageTest, Playwright |
| **Capacity planning & saturation testing** | Load testing, HTTP benchmarking | Gatling, wrk2, Vegeta |
| **Pre-production data realism** | Data generation, service virtualization | Faker, DataFaker, WireMock |

---

## Legend

| Symbol | Meaning |
| ------ | ------- |
| â­ | Widely adopted / reference solution |
| ðŸŸ¢ | Actively maintained |
| ðŸ”µ | Cloud-native / Kubernetes-friendly |
| ðŸŸ  | Commercial / paid offering |
| ðŸ“š | Extensive documentation |
| ðŸš€ | High performance / low overhead |
| ðŸ”— | CI/CD pipeline-ready |
| ðŸ§ª | Developer-friendly / PoC-friendly |
| ðŸ§° | Operations heritage / legacy-friendly |

---

## Load & Stress Testing

*Scriptable load generators for simulating realistic user traffic, validating throughput, and identifying breaking points.*

* **[k6](https://k6.io/)** â­ðŸŸ¢ðŸ”µðŸ“šðŸ”—ðŸ§ª â€” Modern load testing tool with JavaScript ES6 scripting. Developer-friendly, excellent CLI experience, native Prometheus/Grafana integration. Extensible via Go (xk6). Acquired by Grafana Labs. The current reference for shift-left performance testing. [Go] [AGPL-3.0] â€” [GitHub](https://github.com/grafana/k6)
* **[Gatling](https://gatling.io/)** â­ðŸŸ¢ðŸ“šðŸ”—ðŸš€ â€” High-performance load testing framework with Scala/Java/Kotlin DSL. Excellent for complex, protocol-level scenarios. Produces detailed HTML reports. Strong in enterprise Java environments. [Scala/Java] [Apache-2.0] â€” [GitHub](https://github.com/gatling/gatling)
* **[Locust](https://locust.io/)** â­ðŸŸ¢ðŸ“šðŸ§ª â€” Python-based load testing framework. Define user behavior in plain Python code. Distributed mode for scaling. Low barrier to entry for Python teams. [Python] [MIT] â€” [GitHub](https://github.com/locustio/locust)
* **[Apache JMeter](https://jmeter.apache.org/)** â­ðŸŸ¢ðŸ“šðŸ§° â€” Veteran load testing tool with GUI and extensive protocol support (HTTP, JDBC, JMS, LDAP, SOAP, etc.). Massive plugin ecosystem. Heavyweight but unmatched protocol breadth. Still widely used in enterprise testing. [Java] [Apache-2.0] â€” [GitHub](https://github.com/apache/jmeter)
* **[Artillery](https://www.artillery.io/)** ðŸŸ¢ðŸ”µðŸ”—ðŸ§ª â€” Node.js-based load testing toolkit with YAML-based scenarios. Cloud-native, good Kubernetes support. Supports HTTP, WebSocket, Socket.io, and custom engines. [Node.js] [MPL-2.0] â€” [GitHub](https://github.com/artilleryio/artillery)
* **[NBomber](https://nbomber.com/)** ðŸŸ¢ðŸ”—ðŸ§ª â€” Load testing framework for .NET. C#/F# scripting with a focus on developer ergonomics. v5+ requires commercial license for organizational use (v4 remains Apache-2.0). [C#] [Commercial/Apache-2.0] â€” [GitHub](https://github.com/PragmaticFlow/NBomber)
* **[Tsung](https://github.com/processone/tsung)** ðŸš€ðŸ§° â€” Distributed, multi-protocol load testing tool built on Erlang. Handles massive concurrent connections efficiently. Supports HTTP, WebSocket, XMPP, LDAP, and database protocols. Low maintenance activity since 2023. [Erlang] [GPL-2.0]
* **[GoReplay (gor)](https://goreplay.org/)** ðŸŸ¢ðŸš€ â€” Capture and replay production HTTP traffic for load testing and monitoring. Uses real traffic patterns for maximum realism. [Go] [LGPL-3.0] â€” [GitHub](https://github.com/probelabs/goreplay)
* **[Anteon (formerly Ddosify)](https://getanteon.com/)** ðŸ”µðŸ§ª â€” eBPF-based Kubernetes monitoring and performance testing platform with distributed load generation and visual UI. Reduced development activity since mid-2024. [Go] [AGPL-3.0] â€” [GitHub](https://github.com/getanteon/anteon)
* **[Neoload](https://www.tricentis.com/products/performance-testing-neoload)** ðŸŸ ðŸ“š â€” Enterprise performance testing platform with codeless and as-code options. Strong SAP, Citrix, and legacy protocol support. [Commercial]
* **[LoadRunner / OpenText](https://www.opentext.com/products/professional-performance-engineering)** ðŸŸ ðŸ§° â€” The legacy enterprise standard for performance testing. Broad protocol support. Expensive but deeply embedded in many large organizations. [Commercial]

---

## HTTP Benchmarking & Micro-Benchmarking

*High-precision tools for measuring HTTP server throughput, latency distributions, and saturation behavior. Complementary to full load testing â€” focused on raw numbers rather than user scenarios.*

* **[wrk2](https://github.com/giltene/wrk2)** ðŸš€ â€” Constant-throughput HTTP benchmarking tool that produces accurate latency histograms (HdrHistogram). Fixes the coordinated omission problem present in most benchmarking tools. Essential for understanding true tail latency. Stable but not actively maintained (last significant commits ~2019). [C] [Apache-2.0]
* **[wrk](https://github.com/wg/wrk)** ðŸš€ â€” Modern HTTP benchmarking tool with Lua scripting. Fast and simple, but susceptible to coordinated omission. Best used for relative comparisons, not absolute latency measurement. Stable, minimal recent activity. [C] [Apache-2.0]
* **[Vegeta](https://github.com/tsenart/vegeta)** ðŸŸ¢ðŸš€ðŸ”— â€” HTTP load testing tool with constant request rate mode. Clean CLI, library usage in Go, and built-in plotting. Good for saturation testing. [Go] [MIT]
* **[hey](https://github.com/rakyll/hey)** ðŸŸ¢ðŸ§ª â€” Simple HTTP load generator. Successor to Apache Bench (ab). Quick and easy for ad-hoc benchmarks. [Go] [Apache-2.0]
* **[oha](https://github.com/hatoo/oha)** ðŸŸ¢ðŸš€ðŸ§ª â€” HTTP load generator with real-time TUI. Written in Rust for performance. Modern alternative to hey/ab. [Rust] [MIT]
* **[bombardier](https://github.com/codesenberg/bombardier)** ðŸŸ¢ðŸš€ â€” Fast, cross-platform HTTP benchmarking tool with good latency reporting. [Go] [MIT]
* **[hyperfoil](https://hyperfoil.io/)** ðŸŸ¢ðŸ”µðŸš€ â€” Distributed benchmarking framework designed to avoid coordinated omission. Built for microservice architectures. [Java] [Apache-2.0] â€” [GitHub](https://github.com/Hyperfoil/Hyperfoil)

> **A note on coordinated omission:** Many benchmarking tools (including wrk, ab, and hey) report misleadingly optimistic latencies because they don't account for queuing delay when the server is overloaded. Tools like wrk2, Vegeta (constant-rate mode), and Hyperfoil explicitly address this. For serious latency analysis, this matters. See [Gil Tene's talk](https://www.youtube.com/watch?v=lJ8ydIuPFeU) for the full explanation.

---

## API Testing & Contract Testing

*Validating API behavior, correctness, and performance at the service boundary â€” where performance issues often originate and are cheapest to detect.*

* **[Hurl](https://hurl.dev/)** ðŸŸ¢ðŸ”—ðŸ§ª â€” Run and test HTTP requests with plain text. Excellent for API testing in CI pipelines. Supports assertions, captures, and chaining. [Rust] [Apache-2.0] â€” [GitHub](https://github.com/Orange-OpenSource/hurl)
* **[Postman](https://www.postman.com/)** â­ðŸŸ¢ðŸ“šðŸŸ  â€” API development and testing platform. Collection Runner for basic load testing. Newman CLI for CI/CD integration. Ubiquitous in API development. [Commercial/Free tier]
* **[REST-assured](https://rest-assured.io/)** ðŸŸ¢ðŸ“š â€” Java DSL for testing REST APIs. Fluent syntax, integrates with JUnit/TestNG. Standard for Java-based API testing. [Java] [Apache-2.0] â€” [GitHub](https://github.com/rest-assured/rest-assured)
* **[Karate](https://karatelabs.github.io/karate/)** ðŸŸ¢ðŸ“šðŸ§ª â€” BDD-style API testing framework that combines API testing, mocking, and performance testing. Unique Gherkin-like syntax for non-developers. Built-in Gatling integration for load testing. [Java] [MIT] â€” [GitHub](https://github.com/karatelabs/karate)
* **[Step CI](https://stepci.com/)** ðŸŸ¢ðŸ”—ðŸ§ª â€” Open-source API testing and monitoring framework. YAML-based workflow definitions, designed for CI/CD. [TypeScript] [MPL-2.0] â€” [GitHub](https://github.com/stepci/stepci)
* **[Pact](https://pact.io/)** ðŸŸ¢ðŸ”— â€” Contract testing framework for HTTP APIs and messaging. Ensures provider-consumer compatibility without integration tests. Catches breaking changes that cause performance degradation. [Multiple] [MIT] â€” [GitHub](https://github.com/pact-foundation)
* **[Dredd](https://github.com/apiaryio/dredd)** ðŸ”—ðŸ§° â€” API testing from API description documents (OpenAPI, API Blueprint). Validates implementation matches specification. In maintenance mode. [JavaScript] [MIT]

---

## gRPC & Protocol-Specific Testing

*Performance testing beyond HTTP â€” gRPC, WebSocket, message queues, and other protocols that power modern architectures.*

* **[ghz](https://ghz.sh/)** ðŸŸ¢ðŸš€ðŸ”— â€” Simple gRPC benchmarking and load testing tool. Supports unary and streaming RPCs, configurable concurrency, and multiple output formats (CSV, JSON, HTML). [Go] [Apache-2.0] â€” [GitHub](https://github.com/bojand/ghz)
* **[k6 + xk6-grpc](https://github.com/grafana/xk6-grpc)** ðŸŸ¢ðŸ”µðŸ”— â€” k6 extension for gRPC load testing. Scriptable gRPC scenarios with the full k6 ecosystem. [Go] [AGPL-3.0]
* **[k6 + xk6-kafka](https://github.com/mostafa/xk6-kafka)** ðŸŸ¢ðŸ”µðŸ”— â€” k6 extension for Apache Kafka load testing. Produce and consume messages at scale. [Go] [Apache-2.0]
* **[kafka-producer-perf-test / kafka-consumer-perf-test](https://kafka.apache.org/documentation/#basic_ops_producer_consumer_perf)** ðŸŸ¢ðŸ§° â€” Built-in Kafka benchmarking tools for measuring producer and consumer throughput. [Java] [Apache-2.0]
* **[RabbitMQ PerfTest](https://github.com/rabbitmq/rabbitmq-perf-test)** ðŸŸ¢ â€” Official benchmarking tool for RabbitMQ. Measures throughput and latency for publishing and consuming. [Java] [Apache-2.0/MPL-2.0]
* **[k6 + xk6-websockets](https://grafana.com/docs/k6/latest/javascript-api/k6-websockets/)** ðŸŸ¢ðŸ”µðŸ”— â€” Built-in k6 WebSocket support for testing real-time applications, streaming APIs, and bidirectional protocols. [Go] [AGPL-3.0]

---

## Browser & Frontend Performance

*Measuring and optimizing the performance users actually experience â€” Core Web Vitals, rendering, resource loading, and perceived speed.*

* **[Lighthouse](https://developer.chrome.com/docs/lighthouse/)** â­ðŸŸ¢ðŸ“šðŸ”— â€” Google's auditing tool for performance, accessibility, SEO, and best practices. Produces actionable scores and recommendations. Runnable in Chrome DevTools, CLI, or CI. The baseline for web performance assessment. [JavaScript] [Apache-2.0] â€” [GitHub](https://github.com/GoogleChrome/lighthouse)
* **[WebPageTest](https://www.webpagetest.org/)** â­ðŸŸ¢ðŸ“š â€” Deep web performance analysis with filmstrip views, waterfall charts, and multi-location testing. The gold standard for detailed frontend performance diagnosis. Self-hostable. [PHP] [Apache-2.0] â€” [GitHub](https://github.com/catchpoint/WebPageTest)
* **[Playwright](https://playwright.dev/)** â­ðŸŸ¢ðŸ“šðŸ”— â€” Browser automation framework with built-in performance timing APIs. Supports Chromium, Firefox, and WebKit. Excellent for performance testing of single-page applications. [TypeScript] [Apache-2.0] â€” [GitHub](https://github.com/microsoft/playwright)
* **[Sitespeed.io](https://www.sitespeed.io/)** ðŸŸ¢ðŸ”—ðŸ“š â€” Open-source toolkit for monitoring and measuring web performance. Integrates Lighthouse, WebPageTest, and browser-level metrics into dashboards (Grafana). Strong for continuous performance monitoring. [JavaScript] [MIT] â€” [GitHub](https://github.com/sitespeedio/sitespeed.io)
* **[Puppeteer](https://pptr.dev/)** ðŸŸ¢ðŸ“š â€” Chrome DevTools Protocol API for Node.js. Enables programmatic access to Chrome performance traces, network interception, and rendering metrics. [TypeScript] [Apache-2.0] â€” [GitHub](https://github.com/puppeteer/puppeteer)
* **[Yellowlab Tools](https://yellowlab.tools/)** ðŸŸ¢ðŸ§ª â€” Online tool for auditing frontend code quality and performance. Detects heavy JavaScript, CSS complexity, and rendering issues. [JavaScript] [GPL-2.0] â€” [GitHub](https://github.com/YellowLabTools/YellowLabTools)
* **[SpeedCurve](https://www.speedcurve.com/)** ðŸŸ  â€” Continuous frontend performance monitoring with Core Web Vitals tracking, competitive benchmarking, and visual regression. [Commercial]

---

## Service Virtualization & Mocking

*Simulating dependencies to isolate components for targeted performance testing â€” essential when real services are unavailable, expensive, or rate-limited.*

* **[WireMock](https://wiremock.org/)** â­ðŸŸ¢ðŸ“šðŸ”µ â€” Flexible HTTP mock server and service virtualization tool. Supports request matching, stateful behavior, response templating, and fault injection. Runs standalone or embedded in JVM tests. [Java] [Apache-2.0] â€” [GitHub](https://github.com/wiremock/wiremock)
* **[Mountebank](https://github.com/mountebank-testing/mountebank)** ðŸŸ¢ðŸ“š â€” Multi-protocol service virtualization (HTTP, HTTPS, TCP, SMTP). Supports stubs, proxies, and injection. Test doubles over the wire. [JavaScript] [MIT] â€” [GitHub](https://github.com/mountebank-testing/mountebank)
* **[Hoverfly](https://hoverfly.io/)** ðŸŸ¢ðŸ”µ â€” Lightweight service virtualization for API simulation and testing. Capture-and-replay mode for building realistic simulations from production traffic. [Go] [Apache-2.0] â€” [GitHub](https://github.com/SpectoLabs/hoverfly)
* **[MockServer](https://www.mock-server.com/)** ðŸŸ¢ â€” Enables mocking of any HTTP/HTTPS system via expectations. Supports forward and callback actions. [Java] [Apache-2.0] â€” [GitHub](https://github.com/mock-server/mockserver)
* **[Microcks](https://microcks.io/)** ðŸŸ¢ðŸ”µ â€” Open-source tool for API mocking and testing. Imports OpenAPI, AsyncAPI, gRPC, GraphQL, and SOAP contracts. Kubernetes-native. [Java] [Apache-2.0] â€” [GitHub](https://github.com/microcks/microcks)
* **[Toxiproxy](https://github.com/Shopify/toxiproxy)** ðŸŸ¢ðŸ”— â€” TCP proxy for simulating network conditions (latency, bandwidth, connection failures). From Shopify. Invaluable for testing how applications handle degraded networks. *(See also: [Chaos Engineering](#chaos-engineering-fault-injection), [Network Simulation](#network-simulation-traffic-shaping))* [Go] [MIT]

---

## Synthetic Data Generation

*Creating realistic test datasets that reflect production cardinality, distributions, and edge cases â€” one of the most underestimated aspects of performance testing realism.*

* **[Faker](https://fakerjs.dev/)** â­ðŸŸ¢ðŸ“šðŸ§ª â€” Generate realistic fake data (names, addresses, emails, etc.) in JavaScript/TypeScript. Massive locale support. Available in many languages ([Python](https://github.com/joke2k/faker), [Ruby](https://github.com/faker-ruby/faker), [Java â€” use DataFaker](https://github.com/datafaker-net/datafaker), [Go](https://github.com/bxcodec/faker)). [TypeScript] [MIT] â€” [GitHub](https://github.com/faker-js/faker)
* **[DataFaker](https://www.datafaker.net/)** ðŸŸ¢ðŸ“š â€” Modern Java data generation library. Successor to java-faker with better performance, more providers, and expression-based generation. [Java] [Apache-2.0] â€” [GitHub](https://github.com/datafaker-net/datafaker)
* **[Mimesis](https://mimesis.name/)** ðŸŸ¢ðŸš€ â€” High-performance fake data generator for Python. Faster than Faker for bulk generation. Strong locale support. [Python] [MIT] â€” [GitHub](https://github.com/lk-geimfari/mimesis)
* **[Neosync](https://www.neosync.dev/)** ðŸ”µðŸ§ª â€” Open-source platform for anonymizing production data and generating synthetic datasets. Acquired by Grow Therapy in 2025; no longer actively maintained. Still usable as a self-hosted solution. [Go] [MIT] â€” [GitHub](https://github.com/nucleuscloud/neosync)

---

## Database Performance Testing & Benchmarking

*Validating database throughput, query performance, and scalability â€” often the critical bottleneck in application performance.*

* **[HammerDB](https://www.hammerdb.com/)** â­ðŸŸ¢ðŸ“š â€” Open-source database benchmarking tool supporting TPC-C and TPC-H workloads. Supports Oracle, SQL Server, PostgreSQL, MySQL, and MariaDB. Industry standard for database benchmarks. [Tcl] [GPL-2.0] â€” [GitHub](https://github.com/TPC-Council/HammerDB)
* **[sysbench](https://github.com/akopytov/sysbench)** â­ðŸŸ¢ðŸš€ â€” Scriptable multi-threaded benchmark tool. Database benchmarks (OLTP), CPU, memory, and file I/O tests. Standard tool for MySQL/MariaDB performance evaluation. [C] [GPL-2.0]
* **[pgbench](https://www.postgresql.org/docs/current/pgbench.html)** ðŸŸ¢ðŸ§° â€” PostgreSQL built-in benchmarking. Custom scripts for workload simulation. [C] [PostgreSQL License]
* **[YCSB (Yahoo! Cloud Serving Benchmark)](https://github.com/brianfrankcooper/YCSB)** â­ðŸŸ¢ â€” Framework for benchmarking NoSQL and NewSQL databases. Standard workloads (A-F) enable fair comparison across database technologies. [Java] [Apache-2.0]
* **[benchbase (formerly OLTPBench)](https://github.com/cmu-db/benchbase)** ðŸŸ¢ â€” Multi-DBMS benchmarking framework supporting TPC-C, TPC-H, YCSB, and other workloads. From Carnegie Mellon Database Group. [Java] [Apache-2.0]
* **[mysqlslap](https://dev.mysql.com/doc/refman/en/mysqlslap.html)** ðŸ§° â€” MySQL built-in load emulation client. Quick and simple for MySQL-specific benchmarks. [C] [GPL-2.0]

---

## System & Infrastructure Benchmarking

*Measuring raw system performance â€” CPU, memory, disk I/O, network â€” to establish baselines and validate infrastructure before application-level testing.*

* **[fio](https://fio.readthedocs.io/)** â­ðŸŸ¢ðŸš€ â€” The reference tool for I/O benchmarking. Configurable workloads (sequential, random, mixed), multiple I/O engines (libaio, io_uring, etc.). Essential for storage performance evaluation. [C] [GPL-2.0] â€” [GitHub](https://github.com/axboe/fio)
* **[stress-ng](https://github.com/ColinIanKing/stress-ng)** ðŸŸ¢ðŸš€ â€” System stress testing tool covering CPU, memory, I/O, network, and OS-level stressors. 300+ stress methods. Excellent for validating system stability and thermal limits. [C] [GPL-2.0]
* **[Phoronix Test Suite](https://www.phoronix-test-suite.com/)** ðŸŸ¢ðŸ“š â€” Comprehensive benchmarking platform with 500+ test profiles. Automated testing, result comparison, and historical tracking. [PHP] [GPL-3.0] â€” [GitHub](https://github.com/phoronix-test-suite/phoronix-test-suite)
* **[iperf3](https://software.es.net/iperf/)** â­ðŸŸ¢ðŸš€ â€” Network bandwidth measurement tool. TCP/UDP throughput testing between two endpoints. The standard for network performance validation. [C] [BSD-3-Clause] â€” [GitHub](https://github.com/esnet/iperf)

---

## Chaos Engineering & Fault Injection

*Testing system behavior under failure â€” because production will introduce faults you didn't plan for. Performance under degradation is performance that matters.*

* **[Litmus](https://litmuschaos.io/)** â­ðŸŸ¢ðŸ”µðŸ“š â€” CNCF incubating chaos engineering platform for Kubernetes. Extensive experiment library (ChaosHub), GitOps-friendly, Prometheus integration for measuring impact. [Go] [Apache-2.0] â€” [GitHub](https://github.com/litmuschaos/litmus)
* **[Chaos Mesh](https://chaos-mesh.org/)** â­ðŸŸ¢ðŸ”µ â€” CNCF incubating Kubernetes-native chaos engineering platform. Pod, network, I/O, time, and JVM fault injection. Dashboard and scheduling. [Go] [Apache-2.0] â€” [GitHub](https://github.com/chaos-mesh/chaos-mesh)
* **[Gremlin](https://www.gremlin.com/)** ðŸŸ ðŸ“š â€” Enterprise chaos engineering platform. Managed experiments with safety controls (halt conditions). Supports infrastructure, application, and network attacks. [Commercial]
* **[Chaos Monkey](https://netflix.github.io/chaosmonkey/)** â­ðŸŸ¢ â€” Netflix's original chaos tool: randomly terminates instances in production. Pioneered the discipline. [Go] [Apache-2.0] â€” [GitHub](https://github.com/Netflix/chaosmonkey)
* **[Pumba](https://github.com/alexei-led/pumba)** ðŸŸ¢ðŸ”µ â€” Chaos testing for Docker containers. Kill, pause, stop containers and inject network delays/packet loss. Lightweight alternative to full chaos platforms. [Go] [Apache-2.0]
* **[Toxiproxy](https://github.com/Shopify/toxiproxy)** ðŸŸ¢ðŸ”— â€” TCP proxy for simulating network conditions. Programmatic control of latency, bandwidth, and connection failures. Simple, focused, effective. [Go] [MIT]
* **[Steadybit](https://www.steadybit.com/)** ðŸŸ ðŸ”µ â€” Enterprise reliability platform combining chaos engineering with automated resilience validation. [Commercial]
* **[AWS Fault Injection Service](https://aws.amazon.com/fis/)** ðŸŸ ðŸ”µ â€” Managed fault injection for AWS resources. Native integration with AWS services. [Commercial]

---

## Network Simulation & Traffic Shaping

*Simulating real-world network conditions â€” latency, packet loss, bandwidth constraints â€” to test application behavior over imperfect networks.*

* **[tc (Traffic Control)](https://man7.org/linux/man-pages/man8/tc.8.html)** ðŸ§° â€” Linux kernel traffic shaping. Netem qdisc for network emulation (delay, loss, reordering, corruption). The foundational tool. [C] [GPL-2.0]
* **[Toxiproxy](https://github.com/Shopify/toxiproxy)** ðŸŸ¢ðŸ”— â€” Application-level TCP proxy for simulating network conditions. Easier to use than tc for application-level testing. [Go] [MIT]
* **[Comcast](https://github.com/tylertreat/comcast)** ðŸ§ªðŸ§° â€” Simple CLI tool for simulating bad network conditions (packet loss, latency, bandwidth). Wraps tc/pfctl. No longer actively maintained. [Go] [Apache-2.0]
* **[Clumsy](https://jagt.github.io/clumsy/)** ðŸŸ¢ðŸ§ª â€” Windows network condition simulator. Drop, lag, throttle, and reorder packets. [C] [MIT] â€” [GitHub](https://github.com/jagt/clumsy)

---

## CI/CD Integration & Performance Gates

*Embedding performance validation into delivery pipelines â€” making performance feedback continuous rather than episodic.*

* **[k6](https://k6.io/) + [k6-operator](https://github.com/grafana/k6-operator)** â­ðŸŸ¢ðŸ”µðŸ”— â€” Run distributed k6 load tests in Kubernetes via a custom operator. Native threshold-based pass/fail for pipeline gates. [Go] [Apache-2.0]
* **[Gatling Enterprise](https://gatling.io/platform)** ðŸŸ ðŸ”— â€” Managed Gatling execution with CI/CD integrations, team collaboration, and historical comparison. [Commercial]
* **[Lighthouse CI](https://github.com/GoogleChrome/lighthouse-ci)** ðŸŸ¢ðŸ”— â€” Run Lighthouse in CI with performance budgets. Assert on scores, compare against baselines, and track trends. [JavaScript] [Apache-2.0]
* **[Sitespeed.io](https://www.sitespeed.io/)** ðŸŸ¢ðŸ”— â€” Continuous web performance monitoring with CI/CD integration. Performance budgets, Grafana dashboards, and historical tracking. [JavaScript] [MIT] â€” [GitHub](https://github.com/sitespeedio/sitespeed.io)
* **[Taurus](https://gettaurus.org/)** ðŸŸ¢ðŸ”—ðŸ“š â€” Automation wrapper for JMeter, Gatling, Locust, and other tools. YAML-based test configuration, unified reporting. Simplifies CI/CD integration for existing test suites. [Python] [Apache-2.0] â€” [GitHub](https://github.com/Blazemeter/taurus)
* **[Grafana k6 Cloud](https://grafana.com/products/cloud/k6/)** ðŸŸ ðŸ”— â€” Managed k6 execution with CI/CD integration and geographic distribution. *(See [Commercial & SaaS](#commercial-saas-platforms) for details)* [Commercial]
* **[BlazeMeter](https://www.blazemeter.com/)** ðŸŸ ðŸ”— â€” Cloud performance testing with native CI/CD pipeline integration. *(See [Commercial & SaaS](#commercial-saas-platforms) for details)* [Commercial]

---

## Results Analysis & Reporting

*Tools for making sense of performance test results â€” visualization, comparison, trend analysis, and communication.*

* **[Grafana](https://grafana.com/oss/grafana/)** â­ðŸŸ¢ðŸ“š â€” Visualize k6, Gatling, and JMeter results alongside observability data in unified dashboards. The natural integration point between testing and observability. [TypeScript/Go] [AGPL-3.0] â€” [GitHub](https://github.com/grafana/grafana)
* **[k6 HTML Report](https://github.com/benc-uk/k6-reporter)** ðŸŸ¢ðŸ§ª â€” Generate standalone HTML reports from k6 test results. [JavaScript] [MIT]
* **[HdrHistogram](https://github.com/HdrHistogram/HdrHistogram)** ðŸŸ¢ðŸš€ â€” High Dynamic Range Histogram for latency measurement. Captures the full distribution without data loss. Foundation of accurate latency reporting. Available in [Java](https://github.com/HdrHistogram/HdrHistogram), [Go](https://github.com/HdrHistogram/hdrhistogram-go), [C](https://github.com/HdrHistogram/HdrHistogram_c), and more. [Multiple] [BSD-2-Clause/Public Domain]
* **[Gatling Reports](https://gatling.io/)** ðŸŸ¢ â€” Built-in HTML reports with percentile distributions, request/response time series, and active users timeline. Generated automatically after each Gatling run. [Scala]
* **[Apache JMeter Dashboard](https://jmeter.apache.org/usermanual/generating-dashboard.html)** ðŸŸ¢ðŸ§° â€” Built-in HTML dashboard report generator for JMeter. Produces APDEX scores, response time distributions, and throughput analysis. [Java]
* **[Taurus Reporting](https://gettaurus.org/docs/Reporting/)** ðŸŸ¢ðŸ”— â€” Unified reporting across multiple load testing engines. BlazeMeter integration for cloud reporting. [Python]

---

## Commercial & SaaS Platforms

*Integrated performance testing platforms for teams that need managed infrastructure, geographic distribution, collaboration features, and enterprise support. Choosing between SaaS and open-source is often a question of team maturity, scale, and budget â€” not technical capability.*

### Cloud Provider Services

* **[Azure App Testing](https://azure.microsoft.com/en-us/products/app-testing/)** ðŸŸ ðŸ”µðŸ“š â€” Microsoft's managed load testing service (formerly Azure Load Testing, rebranded September 2025). Supports JMeter and Locust natively with multi-region traffic simulation, server-side metrics correlation via Azure Monitor, and CI/CD integration (Azure DevOps, GitHub Actions). Copilot-assisted test authoring. Pay-per-use pricing (~$0.15/VUH). The most complete first-party cloud provider offering for performance testing. [Commercial]
* **[AWS Distributed Load Testing](https://aws.amazon.com/solutions/implementations/distributed-load-testing-on-aws/)** ðŸŸ ðŸ”µ â€” AWS Solutions Implementation that deploys a distributed load testing architecture via CloudFormation (ECS Fargate). Supports JMeter, k6, and Locust scripts. Multi-region execution, scheduling, and baseline comparison. Not a managed SaaS â€” you deploy and pay for the underlying AWS resources. [Commercial] â€” [GitHub](https://github.com/aws-solutions/distributed-load-testing-on-aws)

### Developer-Centric Platforms

* **[Grafana k6 Cloud](https://grafana.com/products/cloud/k6/)** ðŸŸ ðŸ”— â€” Managed k6 execution with geographic distribution across multiple global load zones, real-time result visualization in Grafana, and native CI/CD integration. Performance Insights provide automated analysis of test results. The natural choice for teams already using k6 OSS who need scale and collaboration. [Commercial]
* **[Octoperf](https://octoperf.com/)** ðŸŸ ðŸ”—ðŸ“š â€” SaaS performance testing platform built on JMeter with a modern UI, team collaboration, and trend analysis. European-hosted (GDPR-compliant). Supports JMeter scripts natively with enhanced reporting and distributed load generation. Good fit for teams migrating from JMeter to a managed platform. [Commercial]
* **~~Flood (Tricentis)~~** â€” Cloud load testing platform supporting JMeter, Gatling, and Selenium. **Decommissioned in June 2024.** Tricentis has migrated users to NeoLoad. Listed for historical reference only.

### Enterprise Platforms

* **[BlazeMeter](https://www.blazemeter.com/)** ðŸŸ ðŸ”—ðŸ“š â€” Cloud performance testing supporting JMeter, Gatling, Locust, Selenium, and Playwright. Mock services, API monitoring, and shared workspaces. Part of Perforce. [Commercial]
* **[Neoload (Tricentis)](https://www.tricentis.com/products/performance-testing-neoload)** ðŸŸ ðŸ“š â€” Enterprise performance testing with codeless design, as-code scripting, and RPA-based protocol support. Strong coverage for SAP, Citrix, and legacy protocols. SLA-based analysis and CI/CD integration. [Commercial]
* **[LoadRunner / OpenText](https://www.opentext.com/products/professional-performance-engineering)** ðŸŸ ðŸ§° â€” The legacy enterprise standard with the broadest protocol support in the industry (50+ protocols). LoadRunner Professional (on-premise) and LoadRunner Cloud (SaaS). Deep APM integration. Expensive but deeply embedded in regulated industries. [Commercial]
* **[Gatling Enterprise](https://gatling.io/platform)** ðŸŸ ðŸ”— â€” Managed Gatling with distributed execution, real-time dashboards, historical comparison, and team collaboration. Self-hosted or cloud options. [Commercial]

---

## ðŸ¤– AI-Augmented Performance Testing

*AI is entering the performance testing lifecycle â€” from test generation and workload modeling to results analysis. For the broader perspective on AI in performance engineering, see the [README](@/awesome-performance-engineering/_index.md).*

Key areas where AI delivers value in performance testing today:

* **Test script generation** â€” LLMs generate k6, Gatling, or JMeter scripts from OpenAPI specs, reducing scaffolding effort
* **Workload modeling** â€” AI analyzes production traffic patterns to generate statistically representative load profiles
* **Automated results analysis** â€” ML-based anomaly detection flags regressions that static thresholds miss (gradual degradation, bimodal latency distributions)
* **Natural language querying** â€” asking "what caused the latency spike at minute 12?" instead of navigating dashboards

### Tools & Integrations

* **[k6 Cloud Performance Insights](https://grafana.com/products/cloud/k6/)** ðŸŸ  â€” Automated analysis of k6 test results. Identifies performance issues, correlates metrics, and suggests root causes. [Commercial]
* **[Datadog Synthetic Monitoring + AI](https://docs.datadoghq.com/synthetics/)** ðŸŸ  â€” Synthetic API and browser tests with ML-powered anomaly detection and intelligent alerting. Correlates synthetic test failures with APM traces. [Commercial]
* **[Dynatrace Load Testing Integration](https://docs.dynatrace.com/docs/deliver/test-pipeline-observability)** ðŸŸ  â€” Automated quality gates in CI/CD using AI-based performance evaluation. Davis AI compares test results against baselines and detects anomalies in real time. [Commercial]
* **[Checkly](https://www.checklyhq.com/)** ðŸŸ¢ðŸ”— â€” Monitoring-as-code with Playwright-based synthetic tests. AI-assisted test generation from browsing sessions (beta). [TypeScript] [Free tier/Commercial] â€” [GitHub](https://github.com/checkly/checkly-cli)
* **LLM-assisted test scripting** â€” GitHub Copilot, Claude Code, and similar AI coding assistants are increasingly effective at generating load test scripts from natural language descriptions or API specifications. Not a standalone tool, but a workflow shift worth noting.

### What's Coming Next

* **Adaptive quality gates** â€” AI that learns performance baselines across releases and detects statistically significant regressions beyond static thresholds
* **Self-tuning load scenarios** â€” dynamic adjustment of workload profiles (ramp-up, request mix, data patterns) based on observed system behavior during tests
* **Performance copilots** â€” AI assistants for end-to-end test strategy: design, execution, analysis, and optimization recommendations
* **Predictive impact analysis** â€” given a code change (PR diff), predicting which performance aspects are affected and recommending targeted tests

> **Practitioner's view:** AI is most valuable for eliminating **mechanical work** (script scaffolding, result triage, report generation) and augmenting **analytical tasks** (identifying regressions, correlating signals). It does not replace workload design thinking, system understanding, or architectural judgment. Use AI to generate the first draft, then apply engineering expertise to make it realistic.

---

## Learning Resources

### Books

#### Performance Testing & Methodology

* *The Art of Application Performance Testing* â€” Ian Molyneaux (O'Reilly, 2nd ed. 2014) â€” Complete performance testing lifecycle from strategy to execution, by a 30+ year industry veteran
* *Performance Testing Guidance for Web Applications* â€” Microsoft patterns & practices â€” [Free online](https://learn.microsoft.com/en-us/previous-versions/msp-n-p/bb924375(v=pandp.10))
* *Every Computer Performance Book* â€” Bob Wescott â€” Practical, vendor-neutral performance analysis methodology
* *Performance Solutions* â€” Connie U. Smith, Lloyd G. Williams (Addison-Wesley, 2001) â€” Software Performance Engineering (SPE) methodology: model performance before building

#### Systems Performance

* *Systems Performance* â€” Brendan Gregg (Addison-Wesley, 2nd ed. 2020) â€” The definitive guide to systems performance analysis and methodology
* *Understanding Software Dynamics* â€” Richard L. Sites (Addison-Wesley, 2021) â€” Modern performance profiling with KUtrace, by the DEC Alpha co-architect
* *The Art of Computer Systems Performance Analysis* â€” Raj Jain (Wiley, 1991) â€” Measurement techniques, experimental design, simulation, queueing models â€” the foundational textbook

#### Capacity Planning & Scalability

* *The Art of Capacity Planning* â€” John Allspaw, Arun Kejariwal (O'Reilly, 2nd ed. 2017) â€” Practical capacity planning for web operations
* *Guerrilla Capacity Planning* â€” Neil J. Gunther (Springer, 2007) â€” Universal Scalability Law (USL) and tactical capacity planning

#### JVM & Java

* *Java Performance* â€” Scott Oaks (O'Reilly, 2nd ed. 2020) â€” Comprehensive JVM performance tuning
* *Optimizing Java* â€” Benjamin J. Evans, James Gough, Chris Newland (O'Reilly, 2018) â€” Beyond GC: JIT, bytecode, threading, and cloud performance
* *JVM Performance Engineering* â€” Monica Beckwith (Addison-Wesley, 2024) â€” OpenJDK HotSpot internals, GraalVM, ahead-of-time compilation

#### Web & Frontend

* *High Performance Browser Networking* â€” Ilya Grigorik (O'Reilly) â€” [Free online](https://hpbn.co/) â€” Essential reading for frontend and network performance
* *High Performance Web Sites* â€” Steve Souders (O'Reilly, 2007) â€” The 14 rules that started modern web performance optimization
* *Web Performance in Action* â€” Jeremy L. Wagner (Manning, 2017) â€” HTTP/2, rendering optimization, automated workflows

#### Database Performance

* *SQL Performance Explained* â€” Markus Winand â€” [Free online](https://use-the-index-luke.com/) â€” Vendor-agnostic indexing guide for Oracle, MySQL, PostgreSQL, SQL Server
* *High Performance MySQL* â€” Silvia Botros, Jeremy Tinley (O'Reilly, 4th ed. 2021) â€” MySQL optimization at scale

#### Systems Thinking

* *Performance Engineering* â€” Giuseppe Serazzi (Springer, 2023) â€” [Free / Open Access](https://link.springer.com/book/10.1007/978-3-031-36763-2) â€” Performance evaluation through case studies using Java Modelling Tools (JMT)
* *Thinking in Systems: A Primer* â€” Donella Meadows (Chelsea Green, 2008) â€” Stocks, flows, feedback loops, and leverage points â€” essential for capacity reasoning

### Talks & Video Resources

* [Gil Tene â€” How NOT to Measure Latency](https://www.youtube.com/watch?v=lJ8ydIuPFeU) â€” Essential talk on coordinated omission and latency measurement
* [Emery Berger â€” Performance Matters](https://www.youtube.com/watch?v=r-TLSBdHe1A) â€” Causal profiling and the Coz profiler (Strange Loop 2019)
* [Martin Thompson â€” Mechanical Sympathy](https://www.infoq.com/presentations/mechanical-sympathy/) â€” Understanding hardware for high-performance software
* [Bryan Cantrill â€” The Hurricane's Butterfly](https://www.youtube.com/watch?v=7AO4wz6gI3Q) â€” Debugging pathologically performing systems (Jane Street 2018)
* [Brendan Gregg â€” Blazing Performance with Flame Graphs](https://www.brendangregg.com/) â€” The original flame graph introduction (USENIX LISA 2013)

### Online Resources & Blogs

* [k6 Documentation](https://grafana.com/docs/k6/latest/) â€” Excellent guides, tutorials, and examples for modern load testing
* [Brendan Gregg's Blog](https://www.brendangregg.com/) â€” Deep technical content on systems performance
* [Web.dev â€” Learn Performance](https://web.dev/learn/performance/) â€” Google's comprehensive course on Core Web Vitals and rendering
* [Gatling Academy](https://academy.gatling.io/) â€” Free training courses for Gatling
* [Mechanical Sympathy Blog](http://mechanical-sympathy.blogspot.com) â€” Martin Thompson on low-latency, lock-free systems, and the Disruptor
* [PerfPlanet / Web Performance Calendar](https://calendar.perfplanet.com/) â€” 24 expert articles every December since 2009
* [High Scalability](https://highscalability.com/) â€” Architecture case studies from Netflix, Twitter, Amazon, and beyond
* [CNCF TAG App Delivery](https://github.com/cncf/tag-app-delivery) â€” Cloud-native delivery practices including performance

### Podcasts

* [TestGuild Performance](https://testguild.com/performance-testing/) â€” Weekly podcast on performance testing and SRE (since 2014)
* [PerfBytes](https://www.perfbytes.com/) â€” Performance engineering topics and conference coverage

### Standards & Methodologies

* [USE Method](https://www.brendangregg.com/usemethod.html) â€” Utilization, Saturation, Errors â€” for resource analysis
* [RED Method](https://grafana.com/blog/the-red-method-how-to-instrument-your-services/) â€” Rate, Errors, Duration â€” for service-level analysis
* [Google SRE â€” Testing for Reliability](https://sre.google/sre-book/testing-reliability/) â€” Testing practices from Google SRE
* [Performance Testing Guidance (Microsoft)](https://learn.microsoft.com/en-us/previous-versions/msp-n-p/bb924375(v=pandp.10)) â€” Methodology for web application performance testing
* [Universal Scalability Law (USL)](https://arxiv.org/abs/0808.1431) â€” Neil Gunther's scalability model that subsumes Amdahl's Law
* [W3C Web Performance APIs](https://www.w3.org/webperf/) â€” Navigation Timing, Resource Timing, Long Tasks API, and other browser performance standards

### Benchmark Suites & Reference Workloads

* [TPC Benchmarks](https://www.tpc.org/) â€” Industry-standard OLTP (TPC-C), decision support (TPC-H, TPC-DS), and IoT benchmarks
* [SPEC Benchmarks](https://www.spec.org/) â€” SPECjbb (Java), SPECweb (web servers), SPECpower (energy efficiency)
* [YCSB](https://github.com/brianfrankcooper/YCSB) â€” Yahoo! Cloud Serving Benchmark for NoSQL and cloud databases (3,500+ citations)

### Research & Academic Resources

* [ICPE â€” International Conference on Performance Engineering](https://icpe.spec.org/) â€” Annual ACM/SPEC conference combining research and industry practice
* [ACM SIGMETRICS](https://www.sigmetrics.org/) â€” Premier academic venue for performance evaluation research
* *A Survey on Load Testing of Large-Scale Software Systems* â€” Jiang & Hassan (IEEE TSE, 2015) â€” Comprehensive survey covering load design, execution, and analysis
* *Software Performance AntiPatterns* â€” Smith & Williams (WOSP, 2000) â€” Foundational catalog of recurring performance problems and solutions
* *Hunter: Using Change Point Detection for Performance Regressions* â€” MongoDB (ICPE, 2023) â€” Automated regression detection in CI/CD

---

## Contributing

Contributions are welcome. Please read **[CONTRIBUTING.md](https://github.com/be-next/awesome-performance-engineering/blob/main/CONTRIBUTING.md)** before submitting changes.

## License

Released under **CC0 1.0 Universal**.
